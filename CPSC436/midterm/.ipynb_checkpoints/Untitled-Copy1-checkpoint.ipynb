{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Project: Regularized Logistic Regression with Real Dataset\n",
    "\n",
    "An extension of Logistic Regression with a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "    1. Upload and transform real valued dataset\n",
    "    2. Scale the dataset using z-score normalization\n",
    "    3. Implement regularization (extending compute_cose and compute_gradient functions)\n",
    "    4. Plot the learning curve (cost vs iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages\n",
    "First, we must import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "This dataset is a dataset of diagnostic breast cancer data which includes 30 real values input features. These features encompass many medical attributes about each patient's tumor. Specifically, the 30 real valued features are computed from 10 attributes about each cell in the tumor. For each of the 10 attributes, the mean, standard error, and mean of the three largest values are recorded, resulting in 30 input features for our regression model. In addition to these features, each data point has an id number and a result, whether the tumor was malignant or benign.\n",
    "\n",
    "Here we load this dataset:\n",
    "  - `X_train` contains the 30 real valued input features\n",
    "  - `y_train` is the diagnostic decision\n",
    "      - `y_train = 1` if the patient's tumor was malignant \n",
    "      - `y_train = 0` if the patient's tumor was benign\n",
    "  - Both `X_train` and `y_train` are numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = []\n",
    "    #with open(filename) as fin:\n",
    "    #    data = [line.strip('\\n').split(',') for line in fin.readlines()]\n",
    "    data = np.loadtxt(filename, dtype=str, delimiter=',')\n",
    "\n",
    "    X = data[:,2:].astype(np.float)\n",
    "    y = data[:,1]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "X_train, y_train = load_data(\"./data/wdbc.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "Here is a summary of some of the notation you will encounter, updated for multiple features.  \n",
    "\n",
    "|General <img width=70/> <br />  Notation  <img width=70/> | Description<img width=350/>| Python (if applicable) |\n",
    "|: ------------|: ------------------------------------------------------------||\n",
    "| $a$ | scalar, non bold                                                      ||\n",
    "| $\\mathbf{a}$ | vector, bold                                                 ||\n",
    "| $\\mathbf{A}$ | matrix, bold capital                                         ||\n",
    "| **Regression** |         |    |     |\n",
    "|  $\\mathbf{X}$ | training example maxtrix                  | `X_train` |   \n",
    "|  $\\mathbf{y}$  | training example  targets                | `y_train` \n",
    "|  $\\mathbf{x}^{(i)}$, $y^{(i)}$ | $i_{th}$Training Example | `X[i]`, `y[i]`|\n",
    "| m | number of training examples | `m`|\n",
    "| n | number of features in each example | `n`|\n",
    "|  $\\mathbf{w}$  |  parameter: weight,                       | `w`    |\n",
    "|  $b$           |  parameter: bias                                           | `b`    |     \n",
    "| $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ | The result of the model evaluation at $\\mathbf{x^{(i)}}$ parameterized by $\\mathbf{w},b$: $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+b$  | `f_wb` | \n",
    "\n",
    "\n",
    "### Normalization\n",
    "Here we will use the z score normalization technique to normalize the dataset.\n",
    "\n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To implement z-score normalization, adjust your input values as shown in this formula:\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{4}$$ \n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $Âµ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return X_norm\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm = zscore_normalize_features(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "Here we will extend the compute cost and compute gradient functions to utilize regularization techniques to avoid overfitting.\n",
    "\n",
    "Cost function and regression function for Regularized Logistic Regression:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&J(\\vec{w}, b)=-\\frac{1}{m} \\sum_{i=1}^m\\left[y^{(i)} \\log \\left(f_{\\vec{w}, b}\\left(\\vec{x}^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-f_{\\vec{w}, b}\\left(\\vec{x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2 m} \\sum_{j=1}^n w_j^2\\\\\n",
    "&\\text { repeat }\\{\\\\\n",
    "&w_j=w_j-\\alpha\\left[\\frac{1}{m} \\sum_{i=1}^m\\left[\\left(f_{\\vec{w}, b}\\left(\\vec{x}^{(i)}\\right)-y^{(i)}\\right) x_j^i\\right]+\\frac{\\lambda}{m} w_j\\right]\\\\\n",
    "&b=b-\\alpha\\left[\\frac{1}{m} \\sum_{i=1}^m\\left(f_{\\vec{w}, b}\\left(\\vec{x}^{(i)}\\right)-y^{(i)}\\right)\\right]\\\\\n",
    "&\\}\\\\\n",
    "&\\text { Where } f_{\\vec{w}, b}(\\vec{x})=\\frac{1}{1+e^{-(\\vec{w} \\cdot \\vec{x}+b)}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    g = (1/(1+np.exp(-z)))\n",
    "    return g\n",
    "\n",
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "\n",
    "    total_cost = ((-1/m)*sum_losses(X, y, w, b, m))+((lambda_/(2*m))*sum_of_squared_features(w, n))\n",
    "    \n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "def sum_of_squared_features(w, n):\n",
    "    return sum([w[j]**2 for j in range(n)])\n",
    "\n",
    "def sum_losses(X, y, w, b, m):\n",
    "    return sum([loss(sigmoid(np.dot(w, X[i])+b), y[i]) for i in range(m)])\n",
    "\n",
    "\n",
    "def loss(fwbx, y):\n",
    "    return (y*np.log(fwbx)) + (1-y)*np.log(1-fwbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
